{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. \"Телышев\" - Вариант №2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ваш вариант -  2\n"
     ]
    }
   ],
   "source": [
    "surname = \"Телышев\"  # Ваша фамилия\n",
    "\n",
    "alphabet = 'абвгдеёжзийклмнопрстуфхцчшщъыьэюя'\n",
    "variants = [4, 42, 21, 21, 34,  1, 44, 26, 18, 43, 38, 26, 18, 43,  3, 49, 45,\n",
    "            7, 42, 25,  4,  9, 36, 33, 31, 29,  5, 31,  4, 19, 24, 27, 33]\n",
    "d = dict(zip(alphabet, variants))\n",
    "variant =  sum([d[element] for element in surname.lower()]) % 2 + 1\n",
    "print(\"Ваш вариант - \", variant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Рейтинги"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задача 1. Средний рейтинг\n",
    "```\n",
    "Реализуйте подсчет среднего рейтинга продуктов. Результат сохранить в HDFS в файле \"avg_rating.csv\".\n",
    "Формат каждой записи: ProdId,Rating\n",
    "```"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "hdfs dfs -rm -r -f -skipTrash /user/cloudera/hw_part_1/spark/task_1\n",
    "hdfs dfs -mkdir -p /user/cloudera/hw_part_1/spark/task_1/data\n",
    "\n",
    "hdfs dfs -copyFromLocal /home/cloudera/Desktop/data_samples/samples_100.json \\\n",
    "                        /user/cloudera/hw_part_1/spark/task_1/data/samples_100.json\n",
    "\n",
    "# hdfs dfs -copyFromLocal /home/cloudera/Desktop/data_samples/Electronics.json \\\n",
    "#                         /user/cloudera/hw_part_1/spark/task_1/data/Electronics.json\n",
    "\n",
    "hdfs dfs -ls -R /user/cloudera/hw_part_1/spark/task_1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_products = sc.textFile('/user/cloudera/hw_part_1/spark/task_1/data/samples_100.json')\n",
    "# rdd_products = sc.textFile('/user/cloudera/hw_part_1/spark/task_1/data/Electronics.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('0528881469', 5.0),\n",
       " ('0528881469', 1.0),\n",
       " ('0528881469', 3.0),\n",
       " ('0528881469', 2.0),\n",
       " ('0528881469', 1.0),\n",
       " ('0594451647', 5.0),\n",
       " ('0594451647', 2.0),\n",
       " ('0594451647', 5.0),\n",
       " ('0594451647', 4.0),\n",
       " ('0594451647', 5.0)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def get_product_rating(product_json):\n",
    "    product = json.loads(product_json)\n",
    "    return (product.get('asin'), product.get('overall'))\n",
    "\n",
    "rdd_product_rating = rdd_products.map(lambda product_json: get_product_rating(product_json))\n",
    "rdd_product_rating.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('0972683275', 4.390243902439025), ('0528881469', 2.4), ('0594451647', 4.2), ('0594481813', 4.0)]\n",
      "['0972683275,4.390243902439025', '0528881469,2.4', '0594451647,4.2', '0594481813,4.0']\n"
     ]
    }
   ],
   "source": [
    "rdd_product_avg_rating = (\n",
    "    rdd_product_rating\n",
    "        .aggregateByKey(\n",
    "                (0, 0),\n",
    "                lambda sum_count, rating: (sum_count[0] + rating, sum_count[1] + 1),\n",
    "                lambda sum_count_x, sum_count_y: (sum_count_x[0] + sum_count_y[0],\n",
    "                                                  sum_count_x[1] + sum_count_y[1])\n",
    "            )\n",
    "        .mapValues(lambda sum_count: sum_count[0] / sum_count[1])\n",
    ")\n",
    "print(rdd_product_avg_rating.collect())\n",
    "\n",
    "def to_csv_line(data):\n",
    "    delimiter = ','\n",
    "    fields = []\n",
    "    for item in data:\n",
    "        field = str(item)\n",
    "        if delimiter in field:\n",
    "            field = '\"{}\"'.format(field)\n",
    "        fields.append(field)\n",
    "    return delimiter.join(fields)\n",
    "\n",
    "print(rdd_product_avg_rating.map(to_csv_line).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# product_avg_rating.py\n",
    "import json\n",
    "import sys\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf().setAppName('ProductAvgRating')\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "rdd_products = sc.textFile(sys.argv[1])\n",
    "\n",
    "def get_product_rating(product_json):\n",
    "    product = json.loads(product_json)\n",
    "    return (product.get('asin'), product.get('overall'))\n",
    "\n",
    "rdd_product_rating = rdd_products.map(lambda product_json: get_product_rating(product_json))\n",
    "rdd_product_avg_rating = (\n",
    "    rdd_product_rating\n",
    "        .aggregateByKey(\n",
    "                (0, 0),\n",
    "                lambda sum_count, rating: (sum_count[0] + rating, sum_count[1] + 1),\n",
    "                lambda sum_count_x, sum_count_y: (sum_count_x[0] + sum_count_y[0],\n",
    "                                                  sum_count_x[1] + sum_count_y[1])\n",
    "            )\n",
    "        .mapValues(lambda sum_count: sum_count[0] / sum_count[1])\n",
    ")\n",
    "\n",
    "def to_csv_line(data):\n",
    "    delimiter = ','\n",
    "    fields = []\n",
    "    for item in data:\n",
    "        field = str(item)\n",
    "        if delimiter in field:\n",
    "            field = '\"{}\"'.format(field)\n",
    "        fields.append(field)\n",
    "    return delimiter.join(fields)\n",
    "\n",
    "rdd_product_avg_rating.map(to_csv_line).saveAsTextFile(sys.argv[2])\n",
    "```"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "hdfs dfs -rm -r -f -skipTrash /user/cloudera/hw_part_1/spark/task_1/output\n",
    "\n",
    "spark2-submit --master yarn product_avg_rating.py \\\n",
    "    hdfs:///user/cloudera/hw_part_1/spark/task_1/data/samples_100.json \\\n",
    "    hdfs:///user/cloudera/hw_part_1/spark/task_1/output\n",
    "\n",
    "#spark2-submit --master yarn product_avg_rating.py \\\n",
    "#    hdfs:///user/cloudera/hw_part_1/spark/task_1/data/Electronics.json \\\n",
    "#    hdfs:///user/cloudera/hw_part_1/spark/task_1/output\n",
    "\n",
    "hdfs dfs -ls -R /user/cloudera/hw_part_1/spark/task_1/output\n",
    "\n",
    "hadoop fs -getmerge \\\n",
    "    /user/cloudera/hw_part_1/spark/task_1/output/part-* \\\n",
    "    /home/cloudera/Desktop/data_samples/avg_rating.csv\n",
    "\n",
    "head /home/cloudera/Desktop/data_samples/avg_rating.csv -n 10"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "B00ARSNT7Q,4.29411764706\n",
    "B001UHMVC2,4.55555555556\n",
    "B001WMFXJ8,4.5\n",
    "B002YNY8GI,4.46511627907\n",
    "B008JH59FC,4.22222222222\n",
    "B00C5R8A6W,4.66666666667\n",
    "B002UTBD7S,3.57142857143\n",
    "B0083E7X64,2.83333333333\n",
    "B0009J1I26,3.66666666667\n",
    "B00C7XH7PE,4.71428571429"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задача 2. Добавление наименования продукта\n",
    "```\n",
    "Напишите программу, которая каждому ProdId из \"avg_rating.csv\" ставит в соответстие названием продукта.\n",
    "Результат сохранить в HDFS в файле \"prodname_avg_rating.csv\": ProdId,Name,Rating\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs dfs -rm -r -f -skipTrash /user/cloudera/hw_part_1/spark/task_2\n",
    "hdfs dfs -mkdir -p /user/cloudera/hw_part_1/spark/task_2/data\n",
    "\n",
    "hdfs dfs -copyFromLocal /home/cloudera/Desktop/data_samples/avg_rating.csv \\\n",
    "                        /user/cloudera/hw_part_1/spark/task_2/data/avg_rating.csv\n",
    "\n",
    "hdfs dfs -copyFromLocal /home/cloudera/Desktop/data_samples/samples_100_meta.json \\\n",
    "                        /user/cloudera/hw_part_1/spark/task_2/data/samples_100_meta.json\n",
    "\n",
    "# hdfs dfs -copyFromLocal /home/cloudera/Desktop/data_samples/Electronics_meta.json \\\n",
    "#                         /user/cloudera/hw_part_1/spark/task_2/data/Electronics_meta.json\n",
    "\n",
    "hdfs dfs -ls -R /user/cloudera/hw_part_1/spark/task_2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
